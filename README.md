# llama-inference
A simple implementation of Llama - 1, 2. Llama Architecture built from scratch using PyTorch all the models are built from scratch that includes GQA (Grouped Query Attention) ,  RoPE (Rotary Positional Embeddings) , RMS Norm, FeedForward Block, Encoder (as this is only for Inferencing the model) , SwiGLU (Activation Function).

All Credits to Umar Jamil : https://www.youtube.com/watch?v=oM4VmoabDAI&t=7212s&ab_channel=UmarJamil

![image](https://github.com/viai957/llama-inference/assets/29157342/94da5ad7-2dff-4a8b-a0c1-72376d9d0996)

